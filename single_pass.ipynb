{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(path)\n",
    "from single_pass_v1 import SinglePassV1, Cluster, Document\n",
    "\n",
    "#增加倒排索引\n",
    "class SinglePassV2(SinglePassV1):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document_map = {}#存储文档信息，id-content结构。当然value也可以使用对象存储文档的其他信息。\n",
    "        self.cluster_map = {}#存储簇的信息，id-cluster_object结构。\n",
    "        self.cluster_iindex = {}#word-cluster_ids结构\n",
    "\n",
    "    #对所有文档分词，并生成id\n",
    "    def preprocess(self, document_list):\n",
    "        for i in range(len(document_list)):\n",
    "            doc_id = \"document_\" + str(i)\n",
    "            content = document_list[i]\n",
    "            words = self.get_key_words(content)\n",
    "            document = Document(doc_id, content, words)\n",
    "            self.document_map[doc_id] = document\n",
    "            \n",
    "    #提取文本特征。这里使用文档内频次最高的K个词语。实际应用中可以用TF-IDF\n",
    "    def get_key_words(self, text, K=5):\n",
    "        words = self.get_words(text)\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        keywords = sorted(word_freq.items(), key=lambda x: x[1],reverse=True)[:K]\n",
    "        keywords = list(map(lambda x: x[0], keywords))\n",
    "        return keywords\n",
    "        \n",
    "    def clutering(self):\n",
    "        for doc_id in self.document_map:\n",
    "#             print(doc_id, self.document_map[doc_id])\n",
    "            words = self.document_map[doc_id].features\n",
    "            if_特立独行 =  True\n",
    "            for cluster_id in self.get_cand_clusters(words):\n",
    "                cluster = self.cluster_map[cluster_id]\n",
    "                if self.similar(cluster, self.document_map[doc_id]):\n",
    "                    cluster.add_doc(doc_id)\n",
    "                    if_特立独行 = False\n",
    "                    break\n",
    "            if if_特立独行:\n",
    "                new_cluser_id = \"cluster_\" + str(len(self.cluster_map))\n",
    "                print(new_cluser_id)\n",
    "                new_cluster = Cluster(new_cluser_id, doc_id)\n",
    "                self.cluster_map[new_cluser_id] = new_cluster\n",
    "                \n",
    "                for word in self.document_map[new_cluster.center_doc_id].features:\n",
    "                    if word not in self.cluster_iindex: self.cluster_iindex[word] = []\n",
    "                    self.cluster_iindex[word].append(new_cluser_id)\n",
    "                    \n",
    "    def get_cand_clusters(self, words):\n",
    "        cand_cluster_ids = []\n",
    "        for word in words:\n",
    "            cand_cluster_ids.extend(self.cluster_iindex.get(word, []))\n",
    "        return cand_cluster_ids\n",
    "        \n",
    "    #打印所有簇的简要内容\n",
    "    def show_clusters(self):\n",
    "        for cluster_id in self.cluster_map:\n",
    "            cluster = self.cluster_map[cluster_id]\n",
    "            print(cluster.cluster_id, cluster.center_doc_id, cluster.members)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    docs = [\"我爱北京天安门，天安门上太阳升。\",\n",
    "            \"我要开着火车去北京，看天安门升旗。\",\n",
    "            \"我们的家乡，在希望的田野上。\",\n",
    "            \"我的老家是一片充满希望的田野。\"]\n",
    "    single_passor = SinglePassV2()\n",
    "    single_passor.fit(docs)\n",
    "    single_passor.show_clusters()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
