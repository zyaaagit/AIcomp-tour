{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "sys.path.append('apis')\n",
    "sys.path.append('model')\n",
    "sys.path.append('scrapy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "# import spacy\n",
    "# from spacy.pipeline import EntityRuler\n",
    "# from spacy.tokens import Doc, Span\n",
    "\n",
    "from gdmap import GDmap\n",
    "from pypinyin import pinyin, Style\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Spacy进行实体标注\n",
    "from SpacyNer import SpacyNer\n",
    "sn = SpacyNer()\n",
    "sn.addpipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Huggingface进行实体标注\n",
    "from HugNer import HugBert\n",
    "hb = HugBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data18_path = \"data/2018-2019茂名（含自媒体）.xlsx\"\n",
    "data20_path = \"data/2020-2021茂名（含自媒体）.xlsx\"\n",
    "spider_hotel_path = \"scrapy/hotel_spider.txt\"\n",
    "spider_scenic_path = \"scrapy/scenic_spider.txt\"\n",
    "spider_canteen_path = \"scrapy/canteen_spider.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 合并原始数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data18_hotel = pd.read_excel(data18_path, sheet_name = '酒店评论')\n",
    "data18_scenic = pd.read_excel(data18_path, sheet_name = '景区评论')\n",
    "data18_travel = pd.read_excel(data18_path, sheet_name = '游记攻略')\n",
    "data18_canteen = pd.read_excel(data18_path, sheet_name = '餐饮评论')\n",
    "data18_wx = pd.read_excel(data18_path, sheet_name = '微信公众号新闻')\n",
    "\n",
    "data20_hotel = pd.read_excel(data20_path, sheet_name = '酒店评论')\n",
    "data20_scenic = pd.read_excel(data20_path, sheet_name = '景区评论')\n",
    "data20_travel = pd.read_excel(data20_path, sheet_name = '游记攻略')\n",
    "data20_canteen = pd.read_excel(data20_path, sheet_name = '餐饮评论')\n",
    "data20_wx = pd.read_excel(data20_path, sheet_name = '微信公众号新闻')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel = pd.concat([data18_hotel, data20_hotel], axis = 0)\n",
    "Scenic = pd.concat([data18_scenic, data20_scenic], axis = 0)\n",
    "Travel = pd.concat([data18_travel, data20_travel], axis = 0)\n",
    "Canteen = pd.concat([data18_canteen, data20_canteen], axis = 0)\n",
    "Wx = pd.concat([data18_wx, data20_wx], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 获取爬虫数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_spider = pd.read_csv(spider_hotel_path)\n",
    "scenic_spider = pd.read_csv(spider_scenic_path)\n",
    "canteen_spider = pd.read_csv(spider_canteen_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 酒店产品提取<br>\n",
    "\n",
    "由于酒店评论中几乎没有什么实质性内容，因此仅对酒店名称进行去重处理，得到的结果即为酒店产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel['语料ID'] = Hotel['酒店评论ID'].apply(lambda x: '酒店评论-'+str(x))\n",
    "Hotel['产品名称'] = Hotel['酒店名称']\n",
    "Hotel['内容'] = Hotel['评论内容']\n",
    "Hotel['年份'] = pd.to_datetime(Hotel['评论日期']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_ner = Hotel.iloc[:,-4:].copy()\n",
    "# Hotel_ner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'52连锁酒店(化州店)',\n",
       " 'IU酒店(信宜中兴六路店)',\n",
       " 'IU酒店(茂名人民南路油城大厦店)',\n",
       " 'Q加·茂名大喜假日酒店',\n",
       " '信宜丽晶酒店',\n",
       " '信宜天鹅湖宾馆',\n",
       " '兰欧酒店(信宜绍秀体育馆店)',\n",
       " '凯里亚德酒店(茂名电白万达广场店)',\n",
       " '凯里亚德酒店(茂名高铁站店)',\n",
       " '化州桐趣电影游戏主题酒店',\n",
       " '化州桔龙商务酒店',\n",
       " '化州森思思酒店',\n",
       " '化州汇嘉大酒店',\n",
       " '化州铭丰假日酒店',\n",
       " '华海酒店(茂名高铁站店)',\n",
       " '卓越酒店(茂名万达广场店)',\n",
       " '哈曼酒店(化州火车站店)',\n",
       " '唐华商务酒店(茂名高铁火车站店)',\n",
       " '城市便捷酒店(茂名学府店)',\n",
       " '城市便捷酒店(茂名水东万达广场区政府店)',\n",
       " '城市便捷酒店(茂名滨海新区电城店)',\n",
       " '好莱登商务宾馆(信宜绍秀体育馆店)',\n",
       " '如家酒店·neo(茂名人民路步行街中心店)',\n",
       " '如家酒店·neo(茂名高铁火车站店)',\n",
       " '尚客优连锁酒店(信宜银湖路店)',\n",
       " '尚客优连锁酒店(茂名高铁火车站店)',\n",
       " '星悦酒店(茂名文化广场店)',\n",
       " '柏曼酒店(茂名大道东汇城店)',\n",
       " '柏曼酒店(茂名油城七路店)',\n",
       " '柏高酒店(茂名高铁站店)',\n",
       " '精途酒店(茂名高铁火车站店)',\n",
       " '维也纳3好酒店(茂名沿江大厦店)',\n",
       " '维也纳国际酒店(茂名万达广场店)',\n",
       " '维也纳国际酒店(茂名电白店)',\n",
       " '维也纳国际酒店(茂名高铁火车站店)',\n",
       " '英利商务酒店(化州上街路店)',\n",
       " '茂名万达广场雅斯特国际酒店',\n",
       " '茂名东诚智能公寓',\n",
       " '茂名化州丽登酒店',\n",
       " '茂名华景商务酒店',\n",
       " '茂名华燕宾馆',\n",
       " '茂名卓钰精品酒店',\n",
       " '茂名南越印象岭南文化主题酒店火车站店',\n",
       " '茂名名晟宾馆',\n",
       " '茂名君怡酒店',\n",
       " '茂名君悦商务酒店',\n",
       " '茂名君悦商务酒店人民大道店',\n",
       " '茂名国际大酒店',\n",
       " '茂名大龙湾酒店',\n",
       " '茂名好来登商务酒店',\n",
       " '茂名威利国际酒店',\n",
       " '茂名安途四季酒店',\n",
       " '茂名宏悦大酒店',\n",
       " '茂名市便捷酒店(高铁火车站店)',\n",
       " '茂名御景国际大酒店',\n",
       " '茂名恒运商务酒店',\n",
       " '茂名文华酒店',\n",
       " '茂名文汇酒店',\n",
       " '茂名星城湾国际酒店',\n",
       " '茂名柏丽酒店',\n",
       " '茂名栈泉酒店',\n",
       " '茂名永利之家',\n",
       " '茂名泊愉青年公寓',\n",
       " '茂名海云雁酒店',\n",
       " '茂名海豚酒店',\n",
       " '茂名温德姆至尊酒店',\n",
       " '茂名熹龙国际大酒店',\n",
       " '茂名碧海湾酒店',\n",
       " '茂名红天旅馆',\n",
       " '茂名红金鼎商务宾馆',\n",
       " '茂名茂南高山九树公寓',\n",
       " '茂名荔晶大酒店',\n",
       " '茂名蓝海洋酒店',\n",
       " '茂名观海楼假日酒店',\n",
       " '茂名诚荟酒店',\n",
       " '茂名途遇精品酒店',\n",
       " '茂名金沙商务酒店',\n",
       " '茂名鑫都宾馆',\n",
       " '茂名零六六八公寓',\n",
       " '茂名龙栖湾旅馆',\n",
       " '轻住悦享酒店(茂名高铁南站店)',\n",
       " '高州卡尔顿酒店',\n",
       " '高州明悦宾馆',\n",
       " '高州沿江商务宾馆',\n",
       " '高州简尚精品酒店',\n",
       " '高州顺得商务酒店',\n",
       " '麗枫酒店(茂名水东店)',\n",
       " '麗枫酒店(茂名电白万达广场店)',\n",
       " '麗枫酒店(茂名高铁站店)'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hotel_ner['in_scrapy'] = Hotel_ner['产品名称'].apply(lambda true : if x in )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'茂名君悦商务酒店': 4,\n",
       "         '维也纳国际酒店(茂名电白店)': 6,\n",
       "         '茂名永利之家': 4,\n",
       "         '茂名诚荟酒店': 43,\n",
       "         '茂名华景商务酒店': 1,\n",
       "         '茂名荔晶大酒店': 23,\n",
       "         '好莱登商务宾馆(信宜绍秀体育馆店)': 2,\n",
       "         '茂名海豚酒店': 61,\n",
       "         '茂名南越印象岭南文化主题酒店火车站店': 15,\n",
       "         '茂名卓钰精品酒店': 4,\n",
       "         '化州汇嘉大酒店': 4,\n",
       "         '茂名海云雁酒店': 7,\n",
       "         '茂名威利国际酒店': 4,\n",
       "         '茂名柏丽酒店': 14,\n",
       "         '城市便捷酒店(茂名学府店)': 86,\n",
       "         '茂名观海楼假日酒店': 59,\n",
       "         '茂名国际大酒店': 44,\n",
       "         '尚客优连锁酒店(茂名高铁火车站店)': 9,\n",
       "         '茂名红金鼎商务宾馆': 10,\n",
       "         '维也纳国际酒店(茂名高铁火车站店)': 19,\n",
       "         '卓越酒店(茂名万达广场店)': 13,\n",
       "         '茂名好来登商务酒店': 6,\n",
       "         '52连锁酒店(化州店)': 3,\n",
       "         '轻住悦享酒店(茂名高铁南站店)': 50,\n",
       "         '茂名熹龙国际大酒店': 35,\n",
       "         '维也纳国际酒店(茂名万达广场店)': 52,\n",
       "         '茂名文汇酒店': 1,\n",
       "         '茂名大龙湾酒店': 1,\n",
       "         '茂名华燕宾馆': 20,\n",
       "         '维也纳3好酒店(茂名沿江大厦店)': 12,\n",
       "         '精途酒店(茂名高铁火车站店)': 25,\n",
       "         '茂名万达广场雅斯特国际酒店': 14,\n",
       "         '华海酒店(茂名高铁站店)': 18,\n",
       "         '星悦酒店(茂名文化广场店)': 4,\n",
       "         'Q加·茂名大喜假日酒店': 54,\n",
       "         '茂名宏悦大酒店': 1,\n",
       "         '唐华商务酒店(茂名高铁火车站店)': 4,\n",
       "         '化州铭丰假日酒店': 2,\n",
       "         '茂名温德姆至尊酒店': 2,\n",
       "         '如家酒店·neo(茂名高铁火车站店)': 1,\n",
       "         '茂名途遇精品酒店': 9,\n",
       "         '茂名零六六八公寓': 1,\n",
       "         '茂名栈泉酒店': 6,\n",
       "         '茂名名晟宾馆': 9,\n",
       "         '茂名金沙商务酒店': 1,\n",
       "         '茂名御景国际大酒店': 28,\n",
       "         'IU酒店(信宜中兴六路店)': 1,\n",
       "         '高州简尚精品酒店': 4,\n",
       "         'IU酒店(茂名人民南路油城大厦店)': 19,\n",
       "         '化州桔龙商务酒店': 6,\n",
       "         '茂名鑫都宾馆': 4,\n",
       "         '茂名君悦商务酒店人民大道店': 1,\n",
       "         '柏曼酒店(茂名大道东汇城店)': 13,\n",
       "         '凯里亚德酒店(茂名电白万达广场店)': 31,\n",
       "         '高州顺得商务酒店': 1,\n",
       "         '茂名安途四季酒店': 1,\n",
       "         '如家酒店·neo(茂名人民路步行街中心店)': 1,\n",
       "         '高州明悦宾馆': 1,\n",
       "         '兰欧酒店(信宜绍秀体育馆店)': 1,\n",
       "         '麗枫酒店(茂名高铁站店)': 2,\n",
       "         '麗枫酒店(茂名电白万达广场店)': 68,\n",
       "         '茂名文华酒店': 1,\n",
       "         '信宜天鹅湖宾馆': 1,\n",
       "         '高州卡尔顿酒店': 1,\n",
       "         '柏曼酒店(茂名油城七路店)': 16,\n",
       "         '英利商务酒店(化州上街路店)': 1,\n",
       "         '茂名君怡酒店': 25,\n",
       "         '城市便捷酒店(茂名水东万达广场区政府店)': 2,\n",
       "         '高州沿江商务宾馆': 1,\n",
       "         '茂名恒运商务酒店': 1,\n",
       "         '尚客优连锁酒店(信宜银湖路店)': 2,\n",
       "         '凯里亚德酒店(茂名高铁站店)': 32,\n",
       "         '茂名蓝海洋酒店': 3,\n",
       "         '茂名碧海湾酒店': 1,\n",
       "         '信宜丽晶酒店': 1,\n",
       "         '茂名东诚智能公寓': 5,\n",
       "         '茂名市便捷酒店(高铁火车站店)': 7,\n",
       "         '茂名红天旅馆': 2,\n",
       "         '城市便捷酒店(茂名滨海新区电城店)': 10,\n",
       "         '茂名龙栖湾旅馆': 1,\n",
       "         '茂名泊愉青年公寓': 1,\n",
       "         '茂名化州丽登酒店': 1,\n",
       "         '茂名星城湾国际酒店': 4,\n",
       "         '化州桐趣电影游戏主题酒店': 1,\n",
       "         '柏高酒店(茂名高铁站店)': 10,\n",
       "         '化州森思思酒店': 1,\n",
       "         '茂名茂南高山九树公寓': 1,\n",
       "         '麗枫酒店(茂名水东店)': 15,\n",
       "         '哈曼酒店(化州火车站店)': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(Hotel_ner['产品名称'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_ID = pd.DataFrame(Hotel_ner['产品名称'].drop_duplicates().copy(), columns=['产品名称'])\n",
    "# Hotel_ID['产品ID'] = ['ID' + str(i)  for i in range(1,len(Hotel_ID) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(scrapy_hotel_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    Hotel_set = set(file.read().split())\n",
    "Hotel_set.update(set(Hotel_ID['产品名称']))\n",
    "\n",
    "Hotel_set = list(Hotel_set)\n",
    "Hotel_set.sort(key=lambda keys:[pinyin(i, style=Style.TONE3) for i in keys])\n",
    "\n",
    "Hotel_set = pd.DataFrame(Hotel_set, columns = [\"酒店\"])\n",
    "Hotel_set.to_csv('酒店集合.txt', sep = '\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_ner = pd.merge(Hotel_ner, Hotel_ID, left_on='产品名称', right_on='产品名称', how='left')\n",
    "Hotel_ner.to_csv(\"./ner_c/hotel_ner.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 景区产品提取<br>\n",
    "\n",
    "首先对景区名称进行去重，然后对评论内容中的内容进行提取<br>\n",
    "\n",
    "论文从马蜂窝网站爬取茂名主要热门旅游景点，加入LAC分词工具，与BiLSTM-CRF一起提取其中的景区<br>\n",
    "\n",
    "但LAC只支持python3.7版本及以下，而m1不适配python3.7及以下版本，因此这里换成使用Spacy进行NER实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenic['语料ID'] = Scenic['景区评论ID'].apply(lambda x: '景区评论-'+str(x))\n",
    "Scenic['产品名称'] = Scenic['景区名称']\n",
    "Scenic['内容'] = Scenic['评论内容']\n",
    "Scenic['年份'] = pd.to_datetime(Scenic['评论日期']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenic_ner = Scenic.iloc[:,-4:].copy()\n",
    "# Scenic_ner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "御水古温泉旅游度假区      86\n",
       "放鸡岛生态旅游国际度假区    69\n",
       "茂名森林公园          62\n",
       "高州仙人洞景区         54\n",
       "中国第一滩旅游度假区      41\n",
       "                ..\n",
       "月亮湾碧海蓝天度假邨       1\n",
       "沙扒湾奇乐摩天轮         1\n",
       "天马山              1\n",
       "苏二村              1\n",
       "庐山村              1\n",
       "Name: 产品名称, Length: 113, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scenic_ner['产品名称'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenic_ID = pd.DataFrame(Scenic_ner['产品名称'].drop_duplicates().copy(), columns=['产品名称'])\n",
    "Scenic_ID['产品ID'] = ['ID' + str(i)  for i in range(90, len(Scenic_ID) + 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于在该数据表中，存在部分景区不属于茂名市，应此予以剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"景区爬取.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Scenic_set = set(file.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_outmm = set()\n",
    "map = GDmap()\n",
    "for i in Scenic_ID['产品名称']:\n",
    "    if map.run(i) != \"茂名市\" and map.run(i) != None and i not in Scenic_set:\n",
    "        scene_outmm.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scenic_set.update(set(Scenic_ID['产品名称']) - scene_outmm)\n",
    "\n",
    "\n",
    "Scenic_set = list(Scenic_set)\n",
    "Scenic_set.sort(key=lambda keys:[pinyin(i, style=Style.TONE3) for i in keys])\n",
    "\n",
    "Scenic_set = pd.DataFrame(Scenic_set, columns = [\"景区\"])\n",
    "Scenic_set.to_csv('景区集合.txt', sep = '\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.tokenizer.pkuseg_update_user_dict(Scenic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 648.97it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 474.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3258.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2770.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4332.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2421.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3775.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3830.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2816.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6657.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4136.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3622.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4328.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3266.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3802.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3728.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4744.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7049.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9198.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6657.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3030.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8905.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4185.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4624.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6462.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3289.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 360.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1952.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2922.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5398.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2514.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2416.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3246.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4905.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2816.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1577.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2637.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1476.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5548.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4288.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4253.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3449.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5592.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3876.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8559.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2863.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1461.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1855.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2263.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3865.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8422.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2880.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4215.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3802.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5555.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2032.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5683.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2273.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 429.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2414.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5652.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3806.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1569.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3412.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3734.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3266.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1845.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2110.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6978.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11522.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6364.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3862.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6069.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2571.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4609.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2739.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2427.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1579.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1827.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2283.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6797.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2432.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6034.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5184.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6626.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2114.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1661.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3137.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4766.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2207.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3524.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2398.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8322.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2941.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2549.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8665.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2605.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6797.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1748.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2933.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3504.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3986.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2551.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1461.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1258.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3934.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2123.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4650.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 378.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1957.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3113.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1508.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3788.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3819.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4181.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2519.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5548.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1678.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5489.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4424.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4310.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1742.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2955.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2985.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2841.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2341.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3253.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2983.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1342.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3021.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2557.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2845.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5315.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3802.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4310.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4306.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10782.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2062.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5511.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2552.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3557.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8559.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2371.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2816.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9198.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2786.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 758.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3435.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6887.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5121.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3480.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3575.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8322.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6364.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3758.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2637.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4096.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3276.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2839.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2398.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3530.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4804.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4002.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4447.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2623.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3708.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2926.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5874.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2410.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2444.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3949.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3705.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5315.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1438.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8355.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2336.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7037.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3560.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3492.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3194.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3084.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7397.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5683.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1993.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2126.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2618.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2770.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2631.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2109.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4993.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6413.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3013.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8867.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3030.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1926.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3292.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5518.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4387.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3144.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2019.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3744.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3998.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4284.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2236.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7294.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7294.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4604.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3323.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2949.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1264.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 401.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3379.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1995.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7002.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2924.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2394.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3323.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5592.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1135.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3548.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2359.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3390.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4877.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2513.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 418.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 174.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1890.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1094.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1550.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2141.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2949.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1295.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2976.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3066.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2020.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 987.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2686.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5262.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3214.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4429.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2347.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2212.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4185.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4529.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2785.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7397.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12787.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2517.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2101.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2201.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1324.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2341.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6990.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2880.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3908.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10951.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6069.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3243.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6141.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8322.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2273.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3010.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8793.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4271.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5907.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2267.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2987.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2198.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 189.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5017.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2914.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2995.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3086.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2164.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2049.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2021.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4981.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2049.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10131.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 759.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7037.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 399.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1257.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1416.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2364.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1927.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2208.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4315.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6278.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3412.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3248.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4084.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2888.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2150.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5497.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1324.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6990.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3584.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4660.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2531.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1964.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7294.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2119.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 674.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6887.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2074.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7281.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2695.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2833.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3068.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4604.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2926.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1592.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3077.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5737.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1626.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3153.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3728.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8665.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2947.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3287.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9868.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2786.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3258.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4519.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2577.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6403.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2207.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2688.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6413.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3109.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7449.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3692.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2732.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3919.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3437.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3379.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6403.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2096.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2403.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6909.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2475.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2451.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9686.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6955.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 343.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7345.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4181.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9404.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5256.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7989.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2799.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5159.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4144.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3584.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3412.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4788.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9868.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3050.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2511.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1388.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1770.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2985.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4253.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3536.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3246.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3390.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2693.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1605.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2517.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2137.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4350.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5005.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7061.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5683.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6636.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3819.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9686.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6413.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2983.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 902.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2966.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3144.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3163.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7973.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11881.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4092.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2763.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5047.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7256.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3366.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3068.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 950.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3323.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2123.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4185.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4332.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7767.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2832.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 361.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2251.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4185.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5691.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3310.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13486.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4670.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 646.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4429.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3246.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3075.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3533.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5370.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3030.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2597.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5683.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4934.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4882.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4777.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4670.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3302.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2420.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2680.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10951.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2922.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4529.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2906.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1742.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1089.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4544.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5562.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1109.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4804.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4593.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12985.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6069.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 822.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2753.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7530.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4068.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4036.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 717.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3816.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3390.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5753.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1596.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3387.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2100.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3347.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7397.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3449.26it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3758.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 627.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12483.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6177.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2132.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3788.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8322.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3919.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5489.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3021.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3688.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4788.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 695.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9686.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10034.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1801.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 782.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2122.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2968.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2425.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10782.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3302.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3876.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3830.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3297.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2874.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3192.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2137.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10866.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5121.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7449.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1382.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 858.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 981.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 985.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 927.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2149.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1119.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2278.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 695.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2261.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2659.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5584.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 457.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2146.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2501.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2371.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3986.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10034.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2833.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3923.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2075.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1024.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2814.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 386.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2428.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1783.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7061.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1552.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "Scenic_ner['sn_ner'] = Scenic_ner['内容'].apply(sn.spacy_ner)\n",
    "Scenic_ner['hb_ner'] = Scenic_ner['内容'].apply(hb.ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 餐饮评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>餐饮评论ID</th>\n",
       "      <th>城市</th>\n",
       "      <th>餐饮名称</th>\n",
       "      <th>评论日期</th>\n",
       "      <th>评论内容</th>\n",
       "      <th>标题</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>茂名</td>\n",
       "      <td>盛香烧鹅（东方市场店）</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>很好吃推荐！</td>\n",
       "      <td>主食3选1，有赠品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>茂名</td>\n",
       "      <td>清香面包店（车田街店）</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>超级好吃 老板又好  量又多</td>\n",
       "      <td>水果忌廉夹心蛋糕（二层）1个，约4磅，圆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>茂名</td>\n",
       "      <td>功夫鸡排（光华北路店）</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>好吃，不得不说比门口正对面那家好吃。服务态度也好。炸的皮松软，不会炸得很硬。</td>\n",
       "      <td>小吃6选1，提供免费WiFi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>茂名</td>\n",
       "      <td>茂名浪漫海岸温德姆酒店望海餐厅</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>品种少，冷冻食品多。 7点就不再出食物，难以接受。</td>\n",
       "      <td>自助晚餐（浪漫海岸跨年音乐节）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>茂名</td>\n",
       "      <td>清香面包店（车田街店）</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>味道不错，至少吃起来不腻，并且全家7个人吃还是挺多的了，就比较实惠咯</td>\n",
       "      <td>水果忌廉夹心蛋糕（二层）1个，约4磅，圆</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   餐饮评论ID  城市             餐饮名称        评论日期  \\\n",
       "0    1001  茂名      盛香烧鹅（东方市场店）  2018-01-01   \n",
       "1    1002  茂名      清香面包店（车田街店）  2018-01-01   \n",
       "2    1003  茂名      功夫鸡排（光华北路店）  2018-01-01   \n",
       "3    1004  茂名  茂名浪漫海岸温德姆酒店望海餐厅  2018-01-02   \n",
       "4    1005  茂名      清香面包店（车田街店）  2018-01-03   \n",
       "\n",
       "                                     评论内容                    标题  \n",
       "0                                  很好吃推荐！             主食3选1，有赠品  \n",
       "1                          超级好吃 老板又好  量又多  水果忌廉夹心蛋糕（二层）1个，约4磅，圆  \n",
       "2  好吃，不得不说比门口正对面那家好吃。服务态度也好。炸的皮松软，不会炸得很硬。        小吃6选1，提供免费WiFi  \n",
       "3               品种少，冷冻食品多。 7点就不再出食物，难以接受。       自助晚餐（浪漫海岸跨年音乐节）  \n",
       "4      味道不错，至少吃起来不腻，并且全家7个人吃还是挺多的了，就比较实惠咯  水果忌廉夹心蛋糕（二层）1个，约4磅，圆  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Canteen.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canteen['语料ID'] = Canteen['餐饮评论ID'].apply(lambda x: '餐饮评论-'+str(x))\n",
    "Canteen['产品名称'] = Canteen['餐饮名称']\n",
    "Canteen['内容'] = Canteen['评论内容'] + '\\n' + Canteen['标题']\n",
    "Canteen['年份'] = pd.to_datetime(Canteen['评论日期']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>语料ID</th>\n",
       "      <th>产品名称</th>\n",
       "      <th>内容</th>\n",
       "      <th>年份</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>餐饮评论-1001</td>\n",
       "      <td>盛香烧鹅（东方市场店）</td>\n",
       "      <td>很好吃推荐！\\n主食3选1，有赠品</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>餐饮评论-1002</td>\n",
       "      <td>清香面包店（车田街店）</td>\n",
       "      <td>超级好吃 老板又好  量又多\\n水果忌廉夹心蛋糕（二层）1个，约4磅，圆</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>餐饮评论-1003</td>\n",
       "      <td>功夫鸡排（光华北路店）</td>\n",
       "      <td>好吃，不得不说比门口正对面那家好吃。服务态度也好。炸的皮松软，不会炸得很硬。\\n小吃6选1，...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>餐饮评论-1004</td>\n",
       "      <td>茂名浪漫海岸温德姆酒店望海餐厅</td>\n",
       "      <td>品种少，冷冻食品多。 7点就不再出食物，难以接受。\\n自助晚餐（浪漫海岸跨年音乐节）</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>餐饮评论-1005</td>\n",
       "      <td>清香面包店（车田街店）</td>\n",
       "      <td>味道不错，至少吃起来不腻，并且全家7个人吃还是挺多的了，就比较实惠咯\\n水果忌廉夹心蛋糕（二...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        语料ID             产品名称  \\\n",
       "0  餐饮评论-1001      盛香烧鹅（东方市场店）   \n",
       "1  餐饮评论-1002      清香面包店（车田街店）   \n",
       "2  餐饮评论-1003      功夫鸡排（光华北路店）   \n",
       "3  餐饮评论-1004  茂名浪漫海岸温德姆酒店望海餐厅   \n",
       "4  餐饮评论-1005      清香面包店（车田街店）   \n",
       "\n",
       "                                                  内容    年份  \n",
       "0                                  很好吃推荐！\\n主食3选1，有赠品  2018  \n",
       "1               超级好吃 老板又好  量又多\\n水果忌廉夹心蛋糕（二层）1个，约4磅，圆  2018  \n",
       "2  好吃，不得不说比门口正对面那家好吃。服务态度也好。炸的皮松软，不会炸得很硬。\\n小吃6选1，...  2018  \n",
       "3         品种少，冷冻食品多。 7点就不再出食物，难以接受。\\n自助晚餐（浪漫海岸跨年音乐节）  2018  \n",
       "4  味道不错，至少吃起来不腻，并且全家7个人吃还是挺多的了，就比较实惠咯\\n水果忌廉夹心蛋糕（二...  2018  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Canteen_ner = Canteen.iloc[:,-4:]\n",
    "Canteen_ner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canteen_ID = pd.DataFrame(Canteen_ner['产品名称'].drop_duplicates().copy(), columns=['产品名称'])\n",
    "# Canteen_ID['产品ID'] = ['ID' + str(i)  for i in range(90, len(Canteen_ID) + 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"餐厅爬取.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Canteen_set = set(file.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 餐厅不作地图探讨，否则太多都没有的\n",
    "# canteen_outmm = set()\n",
    "# map = GDmap()\n",
    "# for i in Canteen_ID['产品名称']:\n",
    "#     if map.run(i) != \"茂名市\" and map.run(i) != None and i not in Canteen_set:\n",
    "#         canteen_outmm.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canteen_set.update(set(Canteen_ID['产品名称']))\n",
    "\n",
    "Canteen_set = list(Canteen_set)\n",
    "Canteen_set.sort(key=lambda keys:[pinyin(i, style=Style.TONE3) for i in keys])\n",
    "\n",
    "Canteen_set = pd.DataFrame(Canteen_set, columns = [\"餐厅\"])\n",
    "Canteen_set.to_csv('餐厅集合.txt', sep = '\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Canteen_ner['sn_ner'] = Canteen_ner['内容'].apply(sn.spacy_ner)\n",
    "Canteen_ner['hb_ner'] = Canteen_ner['内容'].apply(hb.ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 游记攻略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>游记ID</th>\n",
       "      <th>城市</th>\n",
       "      <th>游记标题</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>出发时间</th>\n",
       "      <th>出行天数</th>\n",
       "      <th>人物</th>\n",
       "      <th>人均费用</th>\n",
       "      <th>正文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>茂名</td>\n",
       "      <td>漫游中国之三、广东(下)……茂名、惠州、河源、和平、韶关</td>\n",
       "      <td>2018-05-02 07:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1、茂名放鸡岛\\n放鸡岛 （原名湾舟岛，又名汾洲岛）是位于 广东 省 茂名 市 电白 区博贺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>茂名</td>\n",
       "      <td>粤西美食| 茂名，心中的美食城（不定时更新）</td>\n",
       "      <td>2018-10-15 22:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【茂名在哪】\\n我的大 茂名 位于 广东 西部，简称粤西，很多人都不知道 茂名 是哪，所以我...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>茂名</td>\n",
       "      <td>【茂名】三日游记</td>\n",
       "      <td>2018-11-15 11:38</td>\n",
       "      <td>2018-08-25</td>\n",
       "      <td>3 天</td>\n",
       "      <td>一个人</td>\n",
       "      <td>400RMB</td>\n",
       "      <td>行程总览\\n【地点】\\n茂名\\n\\n【时间】\\n3天（实际游玩时间约2天半）\\n\\n【交通】...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>茂名</td>\n",
       "      <td>几分钟和你吃遍茂名美食-粤西茂名家乡发展有你我</td>\n",
       "      <td>2018-11-30 18:12</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1 天</td>\n",
       "      <td>和朋友</td>\n",
       "      <td>50RMB</td>\n",
       "      <td>前言\\n几分钟真的吃不遍 茂名 ，题目只是一个噱头 🔥🔥这是我第一次写家乡的游记 很多店我觉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>茂名</td>\n",
       "      <td>寻找茂名的历史——周末游高州、化州</td>\n",
       "      <td>2018-12-06 08:09</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>2 天</td>\n",
       "      <td>一个人</td>\n",
       "      <td>800RMB</td>\n",
       "      <td>茂名 原本相对于 广州 而言，交通稍差，不适合周末二日游。随着江湛铁路的开通，交通已经不成问...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   游记ID  城市                          游记标题              发布时间        出发时间 出行天数  \\\n",
       "0  1001  茂名  漫游中国之三、广东(下)……茂名、惠州、河源、和平、韶关  2018-05-02 07:46         NaN  NaN   \n",
       "1  1002  茂名        粤西美食| 茂名，心中的美食城（不定时更新）  2018-10-15 22:21         NaN  NaN   \n",
       "2  1003  茂名                      【茂名】三日游记  2018-11-15 11:38  2018-08-25  3 天   \n",
       "3  1004  茂名       几分钟和你吃遍茂名美食-粤西茂名家乡发展有你我  2018-11-30 18:12  2018-11-30  1 天   \n",
       "4  1005  茂名             寻找茂名的历史——周末游高州、化州  2018-12-06 08:09  2018-11-24  2 天   \n",
       "\n",
       "     人物    人均费用                                                 正文  \n",
       "0   NaN     NaN  1、茂名放鸡岛\\n放鸡岛 （原名湾舟岛，又名汾洲岛）是位于 广东 省 茂名 市 电白 区博贺...  \n",
       "1   NaN     NaN  【茂名在哪】\\n我的大 茂名 位于 广东 西部，简称粤西，很多人都不知道 茂名 是哪，所以我...  \n",
       "2   一个人  400RMB  行程总览\\n【地点】\\n茂名\\n\\n【时间】\\n3天（实际游玩时间约2天半）\\n\\n【交通】...  \n",
       "3   和朋友   50RMB  前言\\n几分钟真的吃不遍 茂名 ，题目只是一个噱头 🔥🔥这是我第一次写家乡的游记 很多店我觉...  \n",
       "4   一个人  800RMB  茂名 原本相对于 广州 而言，交通稍差，不适合周末二日游。随着江湛铁路的开通，交通已经不成问...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Travel.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Travel['语料ID'] = Travel['游记ID'].apply(lambda x: '游记攻略-'+str(x))\n",
    "Travel['内容'] = Travel['游记标题'] + '\\n' + Travel['正文']\n",
    "Travel['年份'] = pd.to_datetime(Travel['发布时间']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Travel_ner = Travel[['语料ID','内容','年份']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublink(txt):\n",
    "    return re.sub(r'^.*?\\n\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\n.*?\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n|\\d+\\-\\d+\\-\\d+.*?\\nhttp.*?\\n| ', '', txt)\n",
    "\n",
    "Travel_ner['内容'] = Travel_ner['内容'].apply(sublink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>语料ID</th>\n",
       "      <th>内容</th>\n",
       "      <th>年份</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>游记攻略-1001</td>\n",
       "      <td>漫游中国之三、广东(下)……茂名、惠州、河源、和平、韶关\\n1、茂名放鸡岛\\n放鸡岛（原名湾...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>游记攻略-1002</td>\n",
       "      <td>粤西美食|茂名，心中的美食城（不定时更新）\\n【茂名在哪】\\n我的大茂名位于广东西部，简称粤...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>游记攻略-1003</td>\n",
       "      <td>【茂名】三日游记\\n行程总览\\n【地点】\\n茂名\\n\\n【时间】\\n3天（实际游玩时间约2天...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>游记攻略-1004</td>\n",
       "      <td>几分钟和你吃遍茂名美食-粤西茂名家乡发展有你我\\n前言\\n几分钟真的吃不遍茂名，题目只是一个...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>游记攻略-1005</td>\n",
       "      <td>寻找茂名的历史——周末游高州、化州\\n茂名原本相对于广州而言，交通稍差，不适合周末二日游。随...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>游记攻略-1290</td>\n",
       "      <td>广东周边游｜御水古温泉｜宝藏度假地\\n🏨茂名御水古温泉度假酒店\\n广东茂名宝藏小众度假地～\\...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>游记攻略-1291</td>\n",
       "      <td>99%的人不知道御水古温泉还可以这样玩！\\n🉑️骨灰级御水古温泉3日2夜攻略\\n🉑️御水古温...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>游记攻略-1292</td>\n",
       "      <td>高州行一D1佛山至高州：潘洲公园、观山、高州博物馆、宝光塔、缅茄树、鉴江、瀛洲公园、高凉鼓韵...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>游记攻略-1293</td>\n",
       "      <td>我们去茂名走马观花（广东⑮）\\n走马：路上\\n爷爷是一个很重男轻女的传统广东人，但在看到自己...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>游记攻略-1294</td>\n",
       "      <td>高州行一D4深镇镇&amp;耀新村\\n昨晚夜宿仙人洞附近民宿一仙人阁山庄。\\n山庄为年轻人回乡创业项...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          语料ID                                                 内容    年份\n",
       "0    游记攻略-1001  漫游中国之三、广东(下)……茂名、惠州、河源、和平、韶关\\n1、茂名放鸡岛\\n放鸡岛（原名湾...  2018\n",
       "1    游记攻略-1002  粤西美食|茂名，心中的美食城（不定时更新）\\n【茂名在哪】\\n我的大茂名位于广东西部，简称粤...  2018\n",
       "2    游记攻略-1003  【茂名】三日游记\\n行程总览\\n【地点】\\n茂名\\n\\n【时间】\\n3天（实际游玩时间约2天...  2018\n",
       "3    游记攻略-1004  几分钟和你吃遍茂名美食-粤西茂名家乡发展有你我\\n前言\\n几分钟真的吃不遍茂名，题目只是一个...  2018\n",
       "4    游记攻略-1005  寻找茂名的历史——周末游高州、化州\\n茂名原本相对于广州而言，交通稍差，不适合周末二日游。随...  2018\n",
       "..         ...                                                ...   ...\n",
       "143  游记攻略-1290  广东周边游｜御水古温泉｜宝藏度假地\\n🏨茂名御水古温泉度假酒店\\n广东茂名宝藏小众度假地～\\...  2021\n",
       "144  游记攻略-1291  99%的人不知道御水古温泉还可以这样玩！\\n🉑️骨灰级御水古温泉3日2夜攻略\\n🉑️御水古温...  2021\n",
       "145  游记攻略-1292  高州行一D1佛山至高州：潘洲公园、观山、高州博物馆、宝光塔、缅茄树、鉴江、瀛洲公园、高凉鼓韵...  2021\n",
       "146  游记攻略-1293  我们去茂名走马观花（广东⑮）\\n走马：路上\\n爷爷是一个很重男轻女的传统广东人，但在看到自己...  2021\n",
       "147  游记攻略-1294  高州行一D4深镇镇&耀新村\\n昨晚夜宿仙人洞附近民宿一仙人阁山庄。\\n山庄为年轻人回乡创业项...  2021\n",
       "\n",
       "[294 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Travel_ner['sn_ner'] = Travel_ner['内容'].apply(sn.spacy_ner)\n",
    "Travel_ner['hb_ner'] = Travel_ner['内容'].apply(hb.ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 微信公众号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>文章ID</th>\n",
       "      <th>公众号标题</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>正文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2018，对自己好一点</td>\n",
       "      <td>2018-01-02 17:28</td>\n",
       "      <td>2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>春节机票预订有窍门</td>\n",
       "      <td>2018-01-02 17:28</td>\n",
       "      <td>距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>冬日旅游知多D</td>\n",
       "      <td>2018-01-03 17:32</td>\n",
       "      <td>960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2018冬季暖心之旅</td>\n",
       "      <td>2018-01-03 17:32</td>\n",
       "      <td>长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>关于粤K27618号大客车排气管“喷火”事件的情况说明</td>\n",
       "      <td>2018-01-05 16:57</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   文章ID                        公众号标题              发布时间  \\\n",
       "0  1001                  2018，对自己好一点  2018-01-02 17:28   \n",
       "1  1002                    春节机票预订有窍门  2018-01-02 17:28   \n",
       "2  1003                      冬日旅游知多D  2018-01-03 17:32   \n",
       "3  1004                   2018冬季暖心之旅  2018-01-03 17:32   \n",
       "4  1005  关于粤K27618号大客车排气管“喷火”事件的情况说明  2018-01-05 16:57   \n",
       "\n",
       "                                                  正文  \n",
       "0  2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...  \n",
       "1  距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...  \n",
       "2  960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...  \n",
       "3  长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx['语料ID'] = Wx['文章ID'].apply(lambda x: '微信公众号新闻-'+str(x))\n",
    "Wx['内容'] = Wx['正文'] + '\\n' + Wx['公众号标题']\n",
    "Wx['年份'] = pd.to_datetime(Wx['发布时间']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>语料ID</th>\n",
       "      <th>内容</th>\n",
       "      <th>年份</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>微信公众号新闻-1001</td>\n",
       "      <td>2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>微信公众号新闻-1002</td>\n",
       "      <td>距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>微信公众号新闻-1003</td>\n",
       "      <td>960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>微信公众号新闻-1004</td>\n",
       "      <td>长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>微信公众号新闻-1005</td>\n",
       "      <td>\\n关...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           语料ID                                                 内容    年份\n",
       "0  微信公众号新闻-1001  2017的旅程已经结束\\n2018的未来拉开了帷幕\\n新的一年里，请对自己好一点\\n一辈子很...  2018\n",
       "1  微信公众号新闻-1002  距离春节还有一个多月的时间，在线旅游网站的春节机票销售火爆，部分航线甚至一票难求。在这里分享...  2018\n",
       "2  微信公众号新闻-1003  960万平方公里的祖国大地，四季都有独特美景\\n冬天的旅行也别有一番风味\\n但是冬季的严寒气...  2018\n",
       "3  微信公众号新闻-1004  长按二维码，关注我们\\n中心联系人：林小姐13709649096\\n刘小姐135000781...  2018\n",
       "4  微信公众号新闻-1005                                             \\n关...  2018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wx_ner = Wx.iloc[:,-3:]\n",
    "Wx_ner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx_ner['sn_ner'] = Wx_ner['内容'].apply(sn.spacy_ner)\n",
    "Wx_ner['hb_ner'] = Wx_ner['内容'].apply(hb.ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 合并实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_ner = pd.read_csv(\"./ner_c/hotel_ner.csv\", index_col=0)\n",
    "scenic_ner = pd.read_csv('./ner_c/scenic_ner.csv', index_col=0)\n",
    "travel_ner = pd.read_csv('./ner_c/travel_ner.csv', index_col=0)\n",
    "canteen_ner = pd.read_csv('./ner_c/canteen_ner.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set = set(hotel_ner['产品名称'].drop_duplicates().copy())\n",
    "product_set.update(set(scenic_ner['产品名称'].drop_duplicates().copy()))\n",
    "product_set.update(set(canteen_ner['产品名称'].drop_duplicates().copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>语料ID</th>\n",
       "      <th>产品名称</th>\n",
       "      <th>内容</th>\n",
       "      <th>年份</th>\n",
       "      <th>sn_ner</th>\n",
       "      <th>hb_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>景区评论-1001</td>\n",
       "      <td>中国第一滩旅游度假区</td>\n",
       "      <td>超级美 冬天白日里最是舒服</td>\n",
       "      <td>2018</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>景区评论-1002</td>\n",
       "      <td>广垦（茂名）国家热带农业公园</td>\n",
       "      <td>上了一节深刻的农业科普课，值了，在携程买价格挺合理，当时现场活动价35元。“氧吧”，“花海”...</td>\n",
       "      <td>2018</td>\n",
       "      <td>{'携程', '汕湛高速'}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>景区评论-1003</td>\n",
       "      <td>西江温泉度假村</td>\n",
       "      <td>西江温泉度假村坐落于南国玉都——信宜市郊北界镇西江岸边，西江温泉故此得名，距市中心12公里，...</td>\n",
       "      <td>2018</td>\n",
       "      <td>{'郊北界', '西江温泉度假村', '西江温泉度', '西江温泉'}</td>\n",
       "      <td>{'西江温泉度假村', '西江', '西江温泉'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>景区评论-1004</td>\n",
       "      <td>石根山</td>\n",
       "      <td>爬的很累！感觉还是挺不错的，设施再升级完善。2个多小时才爬到山顶！要是有玻璃栈道就完美了！</td>\n",
       "      <td>2018</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>景区评论-1005</td>\n",
       "      <td>天马山生态旅游区</td>\n",
       "      <td>这次是和家人一起出游，选择天马山也是觉得比较适合老人和小朋友，游玩起来不会太累。其实早在04...</td>\n",
       "      <td>2018</td>\n",
       "      <td>{'天马山'}</td>\n",
       "      <td>{'天马山'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>景区评论-2199</td>\n",
       "      <td>茂名森林公园</td>\n",
       "      <td>茂名森林公园不仅是旅游、观光、休闲、娱乐的好去处，更是展示生物多样性和奇妙性，传播百科知识，...</td>\n",
       "      <td>2021</td>\n",
       "      <td>{'广东省青少年科技教育基地', '广东省森林生态旅游示范基地', '茂名森林公园'}</td>\n",
       "      <td>{'全国林业科普基地', '广东省科普教育基地', '广东省青少年科技教育基地', '茂名森...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>景区评论-2200</td>\n",
       "      <td>湖光岩</td>\n",
       "      <td>湖光岩风景区位于中国大陆最南端湛江市区西南18公里处，被联合国地质专家称为研究地球与地质科学...</td>\n",
       "      <td>2021</td>\n",
       "      <td>{'联合国', '玛珥', '火山', '湖光岩风景区'}</td>\n",
       "      <td>{'联合国', '湖光岩风景区'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>景区评论-2201</td>\n",
       "      <td>鼎龙湾欢乐海岸</td>\n",
       "      <td>吴川市鼎龙湾附近一个免费的摄影基地，好看的场景 真的非常非常多，我因为没带几套衣服，时间也有...</td>\n",
       "      <td>2021</td>\n",
       "      <td>{'龙湾国际度假酒店', '龙湾欢乐世界', '吴川'}</td>\n",
       "      <td>{'鼎龙湾', '鼎龙湾国际度假酒店'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>景区评论-2202</td>\n",
       "      <td>御水古温泉旅游度假区</td>\n",
       "      <td>#广东旅游 #广东旅游攻略 #发现旅途的色彩 #冲啊浪人们 广东茂名旅游景点</td>\n",
       "      <td>2021</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>景区评论-2203</td>\n",
       "      <td>月亮湾滨海度假区</td>\n",
       "      <td>虽然是冬季了，但是广东的阳江依然温暖如春。在阳江市阳西县沙扒镇有个月亮湾，是阳江最美海湾之一...</td>\n",
       "      <td>2021</td>\n",
       "      <td>{'阳江', '亮湾', '月亮湾'}</td>\n",
       "      <td>{'阳江', '月亮湾'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1203 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          语料ID            产品名称  \\\n",
       "0    景区评论-1001      中国第一滩旅游度假区   \n",
       "1    景区评论-1002  广垦（茂名）国家热带农业公园   \n",
       "2    景区评论-1003         西江温泉度假村   \n",
       "3    景区评论-1004             石根山   \n",
       "4    景区评论-1005        天马山生态旅游区   \n",
       "..         ...             ...   \n",
       "659  景区评论-2199          茂名森林公园   \n",
       "660  景区评论-2200             湖光岩   \n",
       "661  景区评论-2201         鼎龙湾欢乐海岸   \n",
       "662  景区评论-2202      御水古温泉旅游度假区   \n",
       "663  景区评论-2203        月亮湾滨海度假区   \n",
       "\n",
       "                                                    内容    年份  \\\n",
       "0                                        超级美 冬天白日里最是舒服  2018   \n",
       "1    上了一节深刻的农业科普课，值了，在携程买价格挺合理，当时现场活动价35元。“氧吧”，“花海”...  2018   \n",
       "2    西江温泉度假村坐落于南国玉都——信宜市郊北界镇西江岸边，西江温泉故此得名，距市中心12公里，...  2018   \n",
       "3        爬的很累！感觉还是挺不错的，设施再升级完善。2个多小时才爬到山顶！要是有玻璃栈道就完美了！  2018   \n",
       "4    这次是和家人一起出游，选择天马山也是觉得比较适合老人和小朋友，游玩起来不会太累。其实早在04...  2018   \n",
       "..                                                 ...   ...   \n",
       "659  茂名森林公园不仅是旅游、观光、休闲、娱乐的好去处，更是展示生物多样性和奇妙性，传播百科知识，...  2021   \n",
       "660  湖光岩风景区位于中国大陆最南端湛江市区西南18公里处，被联合国地质专家称为研究地球与地质科学...  2021   \n",
       "661  吴川市鼎龙湾附近一个免费的摄影基地，好看的场景 真的非常非常多，我因为没带几套衣服，时间也有...  2021   \n",
       "662             #广东旅游 #广东旅游攻略 #发现旅途的色彩 #冲啊浪人们 广东茂名旅游景点  2021   \n",
       "663  虽然是冬季了，但是广东的阳江依然温暖如春。在阳江市阳西县沙扒镇有个月亮湾，是阳江最美海湾之一...  2021   \n",
       "\n",
       "                                          sn_ner  \\\n",
       "0                                          set()   \n",
       "1                                 {'携程', '汕湛高速'}   \n",
       "2            {'郊北界', '西江温泉度假村', '西江温泉度', '西江温泉'}   \n",
       "3                                          set()   \n",
       "4                                        {'天马山'}   \n",
       "..                                           ...   \n",
       "659  {'广东省青少年科技教育基地', '广东省森林生态旅游示范基地', '茂名森林公园'}   \n",
       "660                {'联合国', '玛珥', '火山', '湖光岩风景区'}   \n",
       "661                 {'龙湾国际度假酒店', '龙湾欢乐世界', '吴川'}   \n",
       "662                                        set()   \n",
       "663                          {'阳江', '亮湾', '月亮湾'}   \n",
       "\n",
       "                                                hb_ner  \n",
       "0                                                set()  \n",
       "1                                                set()  \n",
       "2                            {'西江温泉度假村', '西江', '西江温泉'}  \n",
       "3                                                set()  \n",
       "4                                              {'天马山'}  \n",
       "..                                                 ...  \n",
       "659  {'全国林业科普基地', '广东省科普教育基地', '广东省青少年科技教育基地', '茂名森...  \n",
       "660                                  {'联合国', '湖光岩风景区'}  \n",
       "661                               {'鼎龙湾', '鼎龙湾国际度假酒店'}  \n",
       "662                                              set()  \n",
       "663                                      {'阳江', '月亮湾'}  \n",
       "\n",
       "[1203 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenic_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in scenic_ner['sn_ner']:\n",
    "    if i != 'set()':\n",
    "        product_set.update(ast.literal_eval(i))\n",
    "for i in scenic_ner['hb_ner']:\n",
    "    if i != 'set()':\n",
    "        product_set.update(ast.literal_eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in travel_ner['sn_ner']:\n",
    "    if i != 'set()':\n",
    "        product_set.update(ast.literal_eval(i))\n",
    "for i in travel_ner['hb_ner']:\n",
    "    if i != 'set()':\n",
    "        product_set.update(ast.literal_eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set_cut = {item for item in product_set if len(item) > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set_cut = pd.DataFrame(product_set_cut, columns=['产品'])\n",
    "product_set_cut.to_csv('product_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 实体筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去除特殊符号\n",
    "[\\n⑥→【�①②～✨]\n",
    "\n",
    "2. 仅包含数字和字母的去处\n",
    "str.isdigit()\n",
    "str.isalpha()\n",
    "str.isalnum()\n",
    "\n",
    "3. 如果包含一些字符串，则删去\n",
    "\n",
    "4. 如果以下面这个结尾或为倒数第二个字，则去除\n",
    "[\"市\",\"区\",\"县\",\"委\",\"局\",\"路\",\"站\",\"镇\"]\n",
    "\n",
    "5. 如果词性以非名词结尾，则删去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去除特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[⑥【�①②～✨\\n→”。]'\n",
    "def remove_symbol(text):\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 仅包含数字和字母的去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_alnum(text):\n",
    "    pattern = r'^[a-zA-Z0-9]+$'\n",
    "    if re.match(pattern, text) != None:\n",
    "        return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 去除包含停用词的字符串\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutword = [\"省\",\"国道\",\"省道\",\"教育局\",\"体育局\",\"办公室\",\"高速\",\"学校\",\"中学\",\"小学\",\"大学\",\"学院\",\"协会\",\"的\",\"集团\",\"公司\",'，','、','街道','政府','所','网','国大陆','国民革命军','入口','中国同盟会','人民','公交车','南极','湛江','1佛山','10.3','5D玻璃','中国共产党','中国国民党','一中']\n",
    "def del_cutword(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    for word in cutword:\n",
    "        if word in text:\n",
    "            return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 去除以某个词为结尾的字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_last = [\"市\",\"区\",\"县\",\"委\",\"局\",\"路\",\"站\",\"镇\",'部']\n",
    "def del_wordlast(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    for word in word_last:\n",
    "        if text[-1] == word or text[-2] == word:\n",
    "            return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 去除词性以adj、verb、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ckiplab/albert-base-chinese-ner/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))' thrown while requesting HEAD https://huggingface.co/ckiplab/albert-base-chinese-ner/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ckiplab/albert-base-chinese-ner/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/connectionpool.py:996\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    994\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/ssl.py:501\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    498\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    499\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    502\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    503\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    504\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    505\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    506\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    507\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    508\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    509\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1042\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/ssl.py:1310\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1311\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ckiplab/albert-base-chinese-ner/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ws_driver  \u001b[39m=\u001b[39m CkipWordSegmenter(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malbert-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m pos_driver \u001b[39m=\u001b[39m CkipPosTagger(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malbert-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m ner_driver \u001b[39m=\u001b[39m CkipNerChunker(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39malbert-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/ckip_transformers/nlp/driver.py:243\u001b[0m, in \u001b[0;36mCkipNerChunker.__init__\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    239\u001b[0m     model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    241\u001b[0m ):\n\u001b[1;32m    242\u001b[0m     model_name \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_model_name(model))\n\u001b[0;32m--> 243\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model_name\u001b[39m=\u001b[39;49mmodel_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/ckip_transformers/nlp/util.py:66\u001b[0m, in \u001b[0;36mCkipTokenClassification.__init__\u001b[0;34m(self, model_name, tokenizer_name, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     61\u001b[0m     model_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     device: Union[\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name \u001b[39mor\u001b[39;00m model_name)\n\u001b[1;32m     69\u001b[0m     \u001b[39m# Allow passing a customized torch.device.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m hub_kwargs \u001b[39m=\u001b[39m {name: kwargs\u001b[39m.\u001b[39mpop(name) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m hub_kwargs_names \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m kwargs}\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 434\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    435\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    436\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    437\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    438\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    439\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[1;32m    442\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:852\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    851\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 852\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    853\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/configuration_utils.py:565\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    564\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    567\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    618\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    621\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    622\u001b[0m         configuration_file,\n\u001b[1;32m    623\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    624\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    625\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    626\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    627\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    628\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    629\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    630\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    631\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    632\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    635\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/file_download.py:1139\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1140\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1141\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1142\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1143\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1144\u001b[0m         )\n\u001b[1;32m   1145\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1146\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/file_download.py:1471\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1468\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m   1470\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1472\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1473\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1474\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1475\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1476\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1477\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1478\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1479\u001b[0m )\n\u001b[1;32m   1480\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1482\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/utils/_http.py:145\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m thrown while requesting \u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m nb_tries \u001b[39m>\u001b[39m max_retries:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    147\u001b[0m \u001b[39m# Sleep for X seconds\u001b[39;00m\n\u001b[1;32m    148\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRetrying in \u001b[39m\u001b[39m{\u001b[39;00msleep_time\u001b[39m}\u001b[39;00m\u001b[39ms [Retry \u001b[39m\u001b[39m{\u001b[39;00mnb_tries\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmax_retries\u001b[39m}\u001b[39;00m\u001b[39m].\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/huggingface_hub-0.13.1-py3.8.egg/huggingface_hub/utils/_http.py:129\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    128\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/requests/adapters.py:559\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ckiplab/albert-base-chinese-ner/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))"
     ]
    }
   ],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "ws_driver  = CkipWordSegmenter(model=\"albert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"albert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"albert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Nc']]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['下川岛']\n",
    "ws  = ws_driver([text],show_progress = False)\n",
    "pos = pos_driver(ws,show_progress = False)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_pos(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    ws  = ws_driver([text],show_progress = False)\n",
    "    pos = pos_driver(ws,show_progress = False)\n",
    "    if 'P' in pos[0] or 'Nh' in pos[0] :\n",
    "        return None\n",
    "    for word in pos[0]:\n",
    "        if word.startswith('V') or word.startswith('D'):\n",
    "            return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set = pd.read_csv('product_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set['产品'] = product_set['产品'].apply(remove_symbol)\n",
    "product_set['产品'] = product_set['产品'].apply(del_alnum)\n",
    "product_set['产品'] = product_set['产品'].apply(del_cutword)\n",
    "product_set['产品'] = product_set['产品'].apply(del_wordlast)\n",
    "product_set['产品'] = product_set['产品'].apply(del_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set.dropna(subset=['产品'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_set.to_csv('result/product.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
